{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1qpKRKx5sHwqMYvYF6eQGpzhEpBpVfDr3",
      "authorship_tag": "ABX9TyM+eeFdaQfN+dLBAcTsrIXq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marioschuele/masterarbeit/blob/main/Federated_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install efficientnet_pytorch"
      ],
      "metadata": {
        "id": "tbCnFOkYNld3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle -q"
      ],
      "metadata": {
        "id": "MfMl-gALpHDY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import random_split, DataLoader, Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = [5, 5]\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import torchvision\n",
        "#from torchvision import transpose\n",
        "import torchvision.models.efficientnet\n",
        "from torchvision.io import read_image\n",
        "from efficientnet_pytorch import EfficientNet"
      ],
      "metadata": {
        "id": "1DJeGcAIo1dE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTUGhXdOWIDI"
      },
      "outputs": [],
      "source": [
        "# Authenticate, download and unzip dataset\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download yuweisunut/sidd-segmented-intrusion-detection-dataset\n",
        "!unzip sidd-segmented-intrusion-detection-dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directory = 'SIDD'\n",
        "client_id = 0\n",
        "uid = 0\n",
        "\n",
        "for client in os.listdir(directory):\n",
        "  curr_path = f'{directory}/{client}/pcap'\n",
        "\n",
        "  for subdir in os.listdir(curr_path):\n",
        "    curr_path = f'{directory}/{client}/pcap/{subdir}/dataset'\n",
        "    curr_type = subdir[-1:]\n",
        "   \n",
        "    for dayscen in os.listdir(curr_path):\n",
        "      curr_path = f'{directory}/{client}/pcap/{subdir}/dataset/{dayscen}'\n",
        "      \n",
        "      for img in os.listdir(curr_path):\n",
        "        imgs = {}\n",
        "        if dayscen == 'benign':\n",
        "            imgs[uid] = {'id': uid, 'client_id': client_id, 'label': str(0), 'fn': img, 'path': curr_path + '/' + img}\n",
        "        elif dayscen == 'malicious':\n",
        "            imgs[uid] = {'id': uid, 'client_id': client_id, 'label': str(curr_type), 'fn': img, 'path': curr_path + '/' + img}\n",
        "        uid +=1\n",
        "  client_id += 1"
      ],
      "metadata": {
        "id": "ioq9xVQFo5Wn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_df = pd.DataFrame.from_dict(imgs,orient='index')\n",
        "img_df['label'] = img_df['label'].astype(int)\n",
        "img_df.loc[img_df.index[(img_df['label']==3)],'label'] = 2"
      ],
      "metadata": {
        "id": "70mY5FVhpMJH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_df.head()"
      ],
      "metadata": {
        "id": "z816d8fVreFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SIDDdataset(Dataset):\n",
        "  def __init__(self, img_dir):\n",
        "\n",
        "    self.img_dir = img_dir\n",
        "    self.img_labels = img_dir\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "    image = read_image(img_path)\n",
        "\n",
        "  \n",
        "  def __len_(self):\n",
        "    return len(self.img_labels)\n",
        "  "
      ],
      "metadata": {
        "id": "Qm74q_4HqxGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#total_train_size = len(train_dataset)\n",
        "#total_test_size = len(test_dataset)\n",
        "#total_dev_size = len(val_dataset)\n",
        "\n",
        "classes = 3\n",
        "input_dim = 48\n",
        "\n",
        "num_clients = 15\n",
        "rounds = 10\n",
        "batch_size = 32\n",
        "epochs_per_client = 10\n",
        "learning_rate = 1e-2"
      ],
      "metadata": {
        "id": "ezo-mUPrdSHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "def to_device(data, device):\n",
        "    if isinstance(data, (list, tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader(DataLoader):\n",
        "        def __init__(self, dl, device):\n",
        "            self.dl = dl\n",
        "            self.device = device\n",
        "\n",
        "        def __iter__(self):\n",
        "            for batch in self.dl:\n",
        "                yield to_device(batch, self.device)\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.dl)\n",
        "\n",
        "device = get_device()\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kkeh2JXHccSW",
        "outputId": "469b02a6-5556-4b75-e953-b6172d5612a8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = torchvision.models.efficientnet.efficientnet_v2_m"
      ],
      "metadata": {
        "id": "XsQroFy-82R3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FederatedNet(torch.nn.Module):    \n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(1, 20, 7)\n",
        "        self.conv2 = torch.nn.Conv2d(20, 40, 7)\n",
        "        self.maxpool = torch.nn.MaxPool2d(2, 2)\n",
        "        self.flatten = torch.nn.Flatten()\n",
        "        self.linear = torch.nn.Linear(2560, 10)\n",
        "        self.non_linearity = torch.nn.functional.relu\n",
        "        self.track_layers = {'conv1': self.conv1, 'conv2': self.conv2, 'linear': self.linear}\n",
        "    \n",
        "    def forward(self, x_batch):\n",
        "        out = self.conv1(x_batch)\n",
        "        out = self.non_linearity(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.non_linearity(out)\n",
        "        out = self.maxpool(out)\n",
        "        out = self.flatten(out)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "    \n",
        "    def get_track_layers(self):\n",
        "        return self.track_layers\n",
        "\n",
        "    def apply_parameters(self, parameters_dict):\n",
        "        with torch.no_grad():\n",
        "            for layer_name in parameters_dict:\n",
        "                self.track_layers[layer_name].weight.data *= 0\n",
        "                self.track_layers[layer_name].bias.data *= 0\n",
        "                self.track_layers[layer_name].weight.data += parameters_dict[layer_name]['weight']\n",
        "                self.track_layers[layer_name].bias.data += parameters_dict[layer_name]['bias']\n",
        "    \n",
        "    def get_parameters(self):\n",
        "        parameters_dict = dict()\n",
        "        for layer_name in self.track_layers:\n",
        "            parameters_dict[layer_name] = {\n",
        "                'weight': self.track_layers[layer_name].weight.data, \n",
        "                'bias': self.track_layers[layer_name].bias.data\n",
        "            }\n",
        "        return parameters_dict\n",
        "    \n",
        "    def batch_accuracy(self, outputs, labels):\n",
        "        with torch.no_grad():\n",
        "            _, predictions = torch.max(outputs, dim=1)\n",
        "            return torch.tensor(torch.sum(predictions == labels).item() / len(predictions))\n",
        "    \n",
        "    def _process_batch(self, batch):\n",
        "        images, labels = batch\n",
        "        outputs = self(images)\n",
        "        loss = torch.nn.functional.cross_entropy(outputs, labels)\n",
        "        accuracy = self.batch_accuracy(outputs, labels)\n",
        "        return (loss, accuracy)\n",
        "\n",
        "    def fit(self, dataset, epochs, lr, batch_size=128, opt=torch.optim.SGD):\n",
        "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size, shuffle=True), device)\n",
        "        optimizer = opt(self.parameters(), lr)\n",
        "        history = []\n",
        "        for epoch in range(epochs):\n",
        "            losses = []\n",
        "            accs = []\n",
        "            for batch in dataloader:\n",
        "                loss, acc = self._process_batch(batch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                loss.detach()\n",
        "                losses.append(loss)\n",
        "                accs.append(acc)\n",
        "            avg_loss = torch.stack(losses).mean().item()\n",
        "            avg_acc = torch.stack(accs).mean().item()\n",
        "            history.append((avg_loss, avg_acc))\n",
        "        return history\n",
        "\n",
        "    def evaluate(self, dataset, batch_size=128):\n",
        "        dataloader = DeviceDataLoader(DataLoader(dataset, batch_size), device)\n",
        "        losses = []\n",
        "        accs = []\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader:\n",
        "                loss, acc = self._process_batch(batch)\n",
        "                losses.append(loss)\n",
        "                accs.append(acc)\n",
        "        avg_loss = torch.stack(losses).mean().item()\n",
        "        avg_acc = torch.stack(accs).mean().item()\n",
        "        return (avg_loss, avg_acc)\n",
        "    "
      ],
      "metadata": {
        "id": "v_H80mv_b0Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Client:\n",
        "    def __init__(self, client_id, dataset):\n",
        "        self.client_id = client_id\n",
        "        self.dataset = dataset\n",
        "    \n",
        "    def get_dataset_size(self):\n",
        "        return len(self.dataset)\n",
        "    \n",
        "    def get_client_id(self):\n",
        "        return self.client_id\n",
        "    \n",
        "    def train(self, parameters_dict):\n",
        "        net = to_device(FederatedNet(), device)\n",
        "        net.apply_parameters(parameters_dict)\n",
        "        train_history = net.fit(self.dataset, epochs_per_client, learning_rate, batch_size)\n",
        "        print('{}: Loss = {}, Accuracy = {}'.format(self.client_id, round(train_history[-1][0], 4), round(train_history[-1][1], 4)))\n",
        "        return net.get_parameters()"
      ],
      "metadata": {
        "id": "IXID9CJmbgdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clients = [Client('client_' + str(i), client_datasets[i]) for i in range(num_clients)]"
      ],
      "metadata": {
        "id": "t2hQo16DbVqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global_net = to_device(FederatedNet(), device)\n",
        "history = []\n",
        "for i in range(rounds):\n",
        "    print('Start Round {} ...'.format(i + 1))\n",
        "    curr_parameters = global_net.get_parameters()\n",
        "    new_parameters = dict([(layer_name, {'weight': 0, 'bias': 0}) for layer_name in curr_parameters])\n",
        "    for client in clients:\n",
        "        client_parameters = client.train(curr_parameters)\n",
        "        fraction = client.get_dataset_size() / total_train_size\n",
        "        for layer_name in client_parameters:\n",
        "            new_parameters[layer_name]['weight'] += fraction * client_parameters[layer_name]['weight']\n",
        "            new_parameters[layer_name]['bias'] += fraction * client_parameters[layer_name]['bias']\n",
        "    global_net.apply_parameters(new_parameters)\n",
        "    \n",
        "    train_loss, train_acc = global_net.evaluate(train_dataset)\n",
        "    dev_loss, dev_acc = global_net.evaluate(val_dataset)\n",
        "    print('After round {}, train_loss = {}, dev_loss = {}, dev_acc = {}\\n'.format(i + 1, round(train_loss, 4), \n",
        "            round(dev_loss, 4), round(dev_acc, 4)))\n",
        "    history.append((train_acc, dev_acc))"
      ],
      "metadata": {
        "id": "mJiinZIxbN-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot([i + 1 for i in range(len(history))], [history[i][0] for i in range(len(history))], color='r', label='train acc')\n",
        "plt.plot([i + 1 for i in range(len(history))], [history[i][1] for i in range(len(history))], color='b', label='dev acc')\n",
        "plt.legend()\n",
        "plt.title('Training history')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Fjt8bsSklyvE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}