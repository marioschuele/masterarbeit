{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1YrhVVokQyQ2-FCvPOUwFHkBMJcpzdsiY",
      "authorship_tag": "ABX9TyNJt9ziHXaHiU3poTyusOQo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marioschuele/masterarbeit/blob/main/NAS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QfkFhDt0DZXY",
        "outputId": "9721c81c-06c8-4378-c6a5-32a5dc8b9917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "id": "l6P1rwuPRkHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/MiLeNAS/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRXoED-YDrlb",
        "outputId": "0a8be7df-921e-41e0-b2b9-767361be61ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MiLeNAS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF7wBZKiJRBg",
        "outputId": "6d932b9d-fba6-40a7-f0be-f27aeebd8669"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Mar 12 20:51:49 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P0    52W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install graphviz\n",
        "!pip install ptflops\n",
        "!pip install wandb"
      ],
      "metadata": {
        "id": "AlSeVLLDB0ew",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f88bcc2d-0aab-4545-e482-53881caa3b48"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.9/dist-packages (0.10.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ptflops\n",
            "  Downloading ptflops-0.6.9.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from ptflops) (1.13.1+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->ptflops) (4.5.0)\n",
            "Building wheels for collected packages: ptflops\n",
            "  Building wheel for ptflops (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ptflops: filename=ptflops-0.6.9-py3-none-any.whl size=11711 sha256=d572bff5e8bae3969548fd01c4fdd9e5c887e55d7367bafcea1786c092a61a87\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/07/9f/879035d99d7b639bbc564d23fed862a679aee7d1a2dced8c2e\n",
            "Successfully built ptflops\n",
            "Installing collected packages: ptflops\n",
            "Successfully installed ptflops-0.6.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
            "Collecting appdirs>=1.4.3\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.17.0-py2.py3-none-any.whl (189 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.1/189.1 KB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (63.4.3)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=35ab418fa7beb958394f0b1d1c5193e8d9a13d8212528f886f1db5e6fd354173\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, appdirs, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 appdirs-1.4.4 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.17.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login\n",
        "#wandb API: 37feacd21286db2904d6baf9275dc9b32df5b2c5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NHorunGnbOb",
        "outputId": "1cc03a11-ba7b-42b1-d324-1cda90e93899"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "uPj4lT2O4W4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download yuweisunut/sidd-segmented-intrusion-detection-dataset\n",
        "!unzip sidd-segmented-intrusion-detection-dataset"
      ],
      "metadata": {
        "id": "X3rHlAAU4eM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_models = \"./MiLeNAS/models\""
      ],
      "metadata": {
        "id": "ACkdPTwTreba"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python ./MiLeNAS/search_algorithm/train_milenas.py --seed 0 --epochs 3 --batch_size 32 --layers 8 --model_path saved_models --learning_rate 0.025 --weight_decay 0.0003"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjMFwrY8yuTg",
        "outputId": "8eca5562-08a6-46f4-f7a6-1637fc30645f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment dir : search-EXP-20230315-223321\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmario-schuele\u001b[0m (\u001b[33mteam-mario\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.14.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20230315_223322-dgyl7qvk\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mr0-e3-lr0.025-l(1,1)\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/team-mario/uncategorized\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/team-mario/uncategorized/runs/dgyl7qvk\u001b[0m\n",
            "2023-03-15 22:33:23.554 INFO:\tgpus = [0]\n",
            "2023-03-15 22:33:23.587 INFO:\tgpu device = 0\n",
            "2023-03-15 22:33:23.588 INFO:\targs = Namespace(run_id=0, data='../data', batch_size=32, learning_rate=0.025, learning_rate_min=0.001, momentum=0.9, weight_decay=0.0003, report_freq=50, gpu='0', epochs=3, init_channels=16, layers=8, model_path='saved_models', cutout=False, cutout_length=16, drop_path_prob=0.3, save='search-EXP-20230315-223321', seed=0, grad_clip=5, train_portion=0.5, unrolled=False, arch_learning_rate=0.0003, arch_weight_decay=0.001, optimization='DARTS', arch_search_method='DARTS', lambda_train_regularizer=1, lambda_valid_regularizer=1, early_stopping=0, group_id=0, w_update_times=1)\n",
            "2023-03-15 22:33:25.629 INFO:\tparam size = 1.930842MB\n",
            "2023-03-15 22:33:25.641 INFO:\tClient structure identified\n",
            "2023-03-15 22:33:25.665 INFO:\tClient structure identified\n",
            "2023-03-15 22:33:25.688 INFO:\tClient structure identified\n",
            "2023-03-15 22:33:25.714 INFO:\tClient structure identified\n",
            "2023-03-15 22:33:25.738 INFO:\tClient structure identified\n",
            "2023-03-15 22:33:25.759 INFO:\tClient structure identified\n",
            "2023-03-15 22:33:25.780 INFO:\tClient structure identified\n",
            "2023-03-15 22:33:25.801 INFO:\tClient structure identified\n",
            "2023-03-15 22:33:25.826 INFO:\tClient structure identified\n",
            "2023-03-15 22:33:25.849 INFO:\tClient structure identified\n",
            "2023-03-15 22:33:25.870 INFO:\tClient structure identified\n",
            "2023-03-15 22:33:25.892 INFO:\tClient structure identified\n",
            "2023-03-15 22:33:25.916 INFO:\tClient structure identified\n",
            "2023-03-15 22:33:25.938 INFO:\tClient structure identified\n",
            "2023-03-15 22:33:25.960 INFO:\tClient structure identified\n",
            "2023-03-15 22:33:26.036 INFO:\tCreated data frame, length: 36071\n",
            "2023-03-15 22:33:26.036 INFO:\tCreated dataset\n",
            "2023-03-15 22:33:26.038 INFO:\tInitialized training queue\n",
            "2023-03-15 22:33:26.038 INFO:\tInitialized validation queue\n",
            "/usr/local/lib/python3.9/dist-packages/torch/optim/lr_scheduler.py:807: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "2023-03-15 22:33:26.039 INFO:\tepoch 0 lr 2.500000e-02\n",
            "2023-03-15 22:33:30.809 INFO:\ttrain 000 2.546463e+00 6.250000 25.000000\n",
            "2023-03-15 22:35:30.475 INFO:\ttrain 050 9.760027e-01 79.779412 90.073529\n",
            "2023-03-15 22:37:30.842 INFO:\ttrain 100 6.733471e-01 83.075495 94.925743\n",
            "2023-03-15 22:39:31.058 INFO:\ttrain 150 5.405684e-01 85.057947 96.605960\n",
            "2023-03-15 22:41:31.664 INFO:\ttrain 200 4.659376e-01 86.178483 97.450249\n",
            "2023-03-15 22:43:31.283 INFO:\ttrain 250 4.167718e-01 87.176295 97.958167\n",
            "2023-03-15 22:45:30.937 INFO:\ttrain 300 3.825514e-01 87.801080 98.297342\n",
            "2023-03-15 22:47:30.629 INFO:\ttrain 350 3.576800e-01 88.212251 98.539886\n",
            "2023-03-15 22:49:30.818 INFO:\ttrain 400 3.383414e-01 88.552057 98.721945\n",
            "2023-03-15 22:51:30.824 INFO:\ttrain 450 3.236757e-01 88.788803 98.863636\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/./MiLeNAS/search_algorithm/train_milenas.py\", line 459, in <module>\n",
            "    main()\n",
            "  File \"/content/./MiLeNAS/search_algorithm/train_milenas.py\", line 290, in main\n",
            "    train_acc, train_obj, train_loss = train(epoch, train_queue, valid_queue, model, architect, criterion, optimizer, lr)\n",
            "  File \"/content/./MiLeNAS/search_algorithm/train_milenas.py\", line 389, in train\n",
            "    architect.step_milenas(input, target, input_search, target_search, lambda_train_regularizer, lambda_valid_regularizer)\n",
            "  File \"/content/MiLeNAS/search_algorithm/architect.py\", line 68, in step_milenas\n",
            "    logits = self.model(input_train)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/MiLeNAS/search_space/model_search.py\", line 225, in forward\n",
            "    s0, s1 = s1, cell(s0, s1, weights)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/MiLeNAS/search_space/model_search.py\", line 56, in forward\n",
            "    s = sum(self._ops[offset + j](h, weights[offset + j]) for j, h in enumerate(states))\n",
            "  File \"/content/MiLeNAS/search_space/model_search.py\", line 56, in <genexpr>\n",
            "    s = sum(self._ops[offset + j](h, weights[offset + j]) for j, h in enumerate(states))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/MiLeNAS/search_space/model_search.py\", line 24, in forward\n",
            "    return sum(w * op(x) for w, op in zip(weights, self._ops))\n",
            "  File \"/content/MiLeNAS/search_space/model_search.py\", line 24, in <genexpr>\n",
            "    return sum(w * op(x) for w, op in zip(weights, self._ops))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/MiLeNAS/search_space/operations.py\", line 69, in forward\n",
            "    return self.op(x)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py\", line 204, in forward\n",
            "    input = module(input)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\n",
            "    return F.batch_norm(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\", line 2450, in batch_norm\n",
            "    return torch.batch_norm(\n",
            "KeyboardInterrupt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m Press Control-C to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mr0-e3-lr0.025-l(1,1)\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/team-mario/uncategorized/runs/dgyl7qvk\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230315_223322-dgyl7qvk/logs\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 sh MiLeNAS/evaluation/run_eval_cifar10.sh \"0\" GDAS_MIXED_LEVEL2 6 0.030 saved_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sebnlTvFWGUO",
        "outputId": "b1c1655d-2a9b-4e45-e488-327d26cfbfff"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment dir : eval-EXP-20230315-225648\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmario-schuele\u001b[0m (\u001b[33mteam-mario\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.14.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20230315_225649-x3ua0ybe\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mGDAS_MIXED_LEVEL2-lr0.03\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/team-mario/uncategorized\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/team-mario/uncategorized/runs/x3ua0ybe\u001b[0m\n",
            "03/15 10:56:50 PM gpus = [0]\n",
            "03/15 10:56:50 PM gpu device = 0\n",
            "03/15 10:56:50 PM args = Namespace(data='../data', batch_size=32, learning_rate=0.03, learning_rate_min=0.001, momentum=0.9, weight_decay=0.0003, report_freq=50, gpu='0', epochs=6, init_channels=36, layers=20, model_path='saved_models', auxiliary=True, auxiliary_weight=0.4, cutout=True, cutout_length=16, drop_path_prob=0.2, save='eval-EXP-20230315-225648', seed=0, arch='GDAS_MIXED_LEVEL2', grad_clip=5)\n",
            "108 108 36\n",
            "108 144 36\n",
            "144 144 36\n",
            "144 144 36\n",
            "144 144 36\n",
            "144 144 36\n",
            "144 144 72\n",
            "144 288 72\n",
            "288 288 72\n",
            "288 288 72\n",
            "288 288 72\n",
            "288 288 72\n",
            "288 288 72\n",
            "288 288 144\n",
            "288 576 144\n",
            "576 576 144\n",
            "576 576 144\n",
            "576 576 144\n",
            "576 576 144\n",
            "576 576 144\n",
            "03/15 10:56:52 PM param size = 4.033198MB\n",
            "03/15 10:56:52 PM Client structure identified\n",
            "03/15 10:56:52 PM Client structure identified\n",
            "03/15 10:56:52 PM Client structure identified\n",
            "03/15 10:56:52 PM Client structure identified\n",
            "03/15 10:56:52 PM Client structure identified\n",
            "03/15 10:56:52 PM Client structure identified\n",
            "03/15 10:56:52 PM Client structure identified\n",
            "03/15 10:56:52 PM Client structure identified\n",
            "03/15 10:56:52 PM Client structure identified\n",
            "03/15 10:56:52 PM Client structure identified\n",
            "03/15 10:56:52 PM Client structure identified\n",
            "03/15 10:56:52 PM Client structure identified\n",
            "03/15 10:56:52 PM Client structure identified\n",
            "03/15 10:56:52 PM Client structure identified\n",
            "03/15 10:56:52 PM Client structure identified\n",
            "03/15 10:56:52 PM Created data frame\n",
            "/usr/local/lib/python3.9/dist-packages/torch/optim/lr_scheduler.py:807: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "03/15 10:56:52 PM epoch 0 lr 3.000000e-02\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/MiLeNAS/evaluation/train.py\", line 322, in <module>\n",
            "    main()\n",
            "  File \"/content/MiLeNAS/evaluation/train.py\", line 238, in main\n",
            "    train_acc, train_obj = train(train_queue, model, criterion, optimizer)\n",
            "  File \"/content/MiLeNAS/evaluation/train.py\", line 270, in train\n",
            "    logits, logits_aux = model(input)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/MiLeNAS/evaluation/model.py\", line 150, in forward\n",
            "    logits_aux = self.auxiliary_head(s1)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/content/MiLeNAS/evaluation/model.py\", line 79, in forward\n",
            "    x = self.classifier(x.view(x.size(0), -1))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\", line 1194, in _call_impl\n",
            "    return forward_call(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x3072 and 768x10)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: param_size 4.0332\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mGDAS_MIXED_LEVEL2-lr0.03\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/team-mario/uncategorized/runs/x3ua0ybe\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230315_225649-x3ua0ybe/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python MiLeNAS/visualization/visualize.py GDAS_MIXED_LEVEL2"
      ],
      "metadata": {
        "id": "nPxfgMjCgRy8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 ./evaluation/run_eval_cifar10.sh --batch_size 16 --gpu 0 --epochs 5 --learning_rate 0.030 --model_path saved_models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3TUdH7w51ko",
        "outputId": "1109fff8-f310-4e9d-c72c-c3f1f02635eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: cifar\n",
            "       [-h]\n",
            "       [--data DATA]\n",
            "       [--batch_size BATCH_SIZE]\n",
            "       [--learning_rate LEARNING_RATE]\n",
            "       [--learning_rate_min LEARNING_RATE_MIN]\n",
            "       [--momentum MOMENTUM]\n",
            "       [--weight_decay WEIGHT_DECAY]\n",
            "       [--report_freq REPORT_FREQ]\n",
            "       [--gpu GPU]\n",
            "       [--epochs EPOCHS]\n",
            "       [--init_channels INIT_CHANNELS]\n",
            "       [--layers LAYERS]\n",
            "       [--model_path MODEL_PATH]\n",
            "       [--auxiliary]\n",
            "       [--auxiliary_weight AUXILIARY_WEIGHT]\n",
            "       [--cutout]\n",
            "       [--cutout_length CUTOUT_LENGTH]\n",
            "       [--drop_path_prob DROP_PATH_PROB]\n",
            "       [--save SAVE]\n",
            "       [--seed SEED]\n",
            "       [--arch ARCH]\n",
            "       [--grad_clip GRAD_CLIP]\n",
            "cifar: error: argument --gpu: expected one argument\n"
          ]
        }
      ]
    }
  ]
}