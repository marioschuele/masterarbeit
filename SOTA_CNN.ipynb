{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marioschuele/masterarbeit/blob/main/SOTA_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oyXK1IHJTJvM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages\n",
        "! pip install kaggle -q"
      ],
      "metadata": {
        "id": "-0faGcpx2Q4H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download and unzip Kaggle dataset\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets download yuweisunut/sidd-segmented-intrusion-detection-dataset\n",
        "!unzip sidd-segmented-intrusion-detection-dataset"
      ],
      "metadata": {
        "id": "I1v8_DVqThpN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1310ff55-1b06-4287-d3e7-051ac52d325f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "sidd-segmented-intrusion-detection-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  sidd-segmented-intrusion-detection-dataset.zip\n",
            "replace SIDD/n005/local.npy? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "\n",
            "N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directory = 'SIDD'\n",
        "\n",
        "imgs = {}\n",
        "uid = 0\n",
        "\n",
        "for client in os.listdir(directory):\n",
        "  curr_path = f'{directory}/{client}/pcap'\n",
        "\n",
        "  for subdir in os.listdir(curr_path):\n",
        "    curr_path = f'{directory}/{client}/pcap/{subdir}/dataset'\n",
        "    curr_type = subdir[-1:]\n",
        "    \n",
        "    for dayscen in os.listdir(curr_path):\n",
        "      curr_path = f'{directory}/{client}/pcap/{subdir}/dataset/{dayscen}'\n",
        "      #+print(dayscen)\n",
        "      for img in os.listdir(curr_path):\n",
        "        if dayscen == 'benign':\n",
        "            imgs[uid] = {'id': uid, 'label': str(0), 'fn': img, 'path': curr_path + '/' + img}\n",
        "        elif dayscen == 'malicious':\n",
        "            imgs[uid] = {'id': uid, 'label': str(curr_type), 'fn': img, 'path': curr_path + '/' + img}\n",
        "        uid +=1\n"
      ],
      "metadata": {
        "id": "GdJxhSGqYIK2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_df = pd.DataFrame.from_dict(imgs,orient='index')\n",
        "img_df['label'] = img_df['label'].astype(int)\n",
        "img_df.loc[img_df.index[(img_df['label']==3)],'label'] = 2"
      ],
      "metadata": {
        "id": "_LImBC0Fd44n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _parse_function(filename, label):\n",
        "    image_string = tf.io.read_file(filename)\n",
        "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
        "    image = tf.cast(image_decoded, tf.float32)\n",
        "    return image, label\n",
        "\n",
        "file_paths = img_df.path\n",
        "file_labels = img_df[\"label\"]\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((file_paths, file_labels))\n",
        "\n",
        "dataset = dataset.map(_parse_function)\n",
        "dataset = dataset.batch(32)\n",
        "\n",
        "#iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
        "#images, labels = iterator.get_next()\n",
        "#labels\n"
      ],
      "metadata": {
        "id": "pOXjJ1yUWisg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Distribute dataset into train, validation and test set\n",
        "def get_dataset_partitions_tf(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000):\n",
        "    assert (train_split + test_split + val_split) == 1\n",
        "    \n",
        "    if shuffle:\n",
        "        # Specify seed to always have the same split distribution between runs\n",
        "        ds = ds.shuffle(shuffle_size, seed=12)\n",
        "    \n",
        "    train_size = int(train_split * ds_size)\n",
        "    val_size = int(val_split * ds_size)\n",
        "    \n",
        "    train_ds = ds.take(train_size)    \n",
        "    val_ds = ds.skip(train_size).take(val_size)\n",
        "    test_ds = ds.skip(train_size).skip(val_size)\n",
        "\n",
        "    print('Size of {}:  {}'.format('train ds', str(train_ds.cardinality().numpy())))\n",
        "    print('Size of {}:  {}'.format('val ds', str(val_ds.cardinality().numpy())))\n",
        "    print('Size of {}:  {}'.format('test ds', str(test_ds.cardinality().numpy())))\n",
        "    \n",
        "    return train_ds, val_ds, test_ds\n",
        "\n",
        "train_ds, val_ds, test_ds = get_dataset_partitions_tf(ds=dataset, ds_size=dataset.cardinality().numpy(), train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=10000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIwxu1-st_Xr",
        "outputId": "f7dfcbda-0a02-4cec-dc3b-73bb2d31a00d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of train ds:  34640\n",
            "Size of val ds:  4330\n",
            "Size of test ds:  4330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.applications.EfficientNetV2M(\n",
        "    include_top=True,\n",
        "    weights=None,\n",
        "    input_tensor=None,\n",
        "    input_shape=None,\n",
        "    pooling=None,\n",
        "    classes=3,\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.SGD(),  \n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']        #keras.metrics.sparse_categorical_accuracy\n",
        ")\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    x=train_ds,\n",
        "    epochs=10,\n",
        "    validation_data=val_ds\n",
        ")\n",
        "\n",
        "evaluation = model.evaluate(test_ds)"
      ],
      "metadata": {
        "id": "ByPsAlobUqyo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot accuracy of model\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "#Plot loss of model\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "gL_bU9kaLWa4",
        "outputId": "d5071adf-6611-42e6-8c72-a0b7954410b7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-c5faf7e02277>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Plot accuracy of model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = ['n005', 'n006', 'n008', 'n031', 'n034', 'n035', 'n036', 'n038', 'n041', 'n045', 'n046', 'n047', 'n048', 'n053', 'n056']\n",
        "classes = [0,1,3]#['Benign', 'Server Message Block', 'TCP SYN flood attack']\n",
        "\n",
        "image_size = (384,384)\n",
        "batch_size = None\n",
        "\n",
        "train_datagen = ImageDataGenerator(validation_split=0.2)\n",
        "train_batches = train_datagen.flow_from_dataframe(\n",
        "    img_df,\n",
        "    x_col=\"path\",\n",
        "    y_col=\"type\",\n",
        "    target_size = image_size,\n",
        "    batch_size = batch_size,\n",
        "    class_mode = 'categorical',\n",
        "    subset = 'training'\n",
        ")\n",
        "\n",
        "validation_batches = train_datagen.flow_from_dataframe(\n",
        "    img_df,\n",
        "    x_col=\"path\",\n",
        "    y_col=\"type\",\n",
        "    target_size = image_size,\n",
        "    batch_size = batch_size,\n",
        "    class_mode = 'categorical',\n",
        "    subset = 'validation'\n",
        ")\n",
        "\n",
        "test_batches = train_datagen.flow_from_dataframe(\n",
        "    img_df,\n",
        "    x_col=\"path\",\n",
        "    y_col=\"type\",\n",
        "    target_size = image_size,\n",
        "    batch_size = batch_size,\n",
        "    class_mode = 'categorical',\n",
        "    shuffle = False,\n",
        "    subset = 'validation'\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0KxNdQNnl3O",
        "outputId": "645d4d86-be64-42c8-c002-24d905b02a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1108468 validated image filenames belonging to 3 classes.\n",
            "Found 277116 validated image filenames belonging to 3 classes.\n",
            "Found 277116 validated image filenames belonging to 3 classes.\n"
          ]
        }
      ]
    }
  ]
}